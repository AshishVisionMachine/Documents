Feature engineering:
 It is the process of using domain knowledge of the data to create features that make machine learning algorithms work
 
 I will be listing out a total of 15 features that we can use
1. Number of Characters
def count_chars(text):
    return len(text)

2. Number of words

def count_words(text):
    return len(text.split())
	
3. Number of capital characters

def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count
	
4. Number of capital words

def count_capital_words(text):
    return sum(map(str.isupper,text.split()))
	
5. Count the number of punctuations

def count_punctuations(text):
    punctuations='!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~'
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d 
	
6. Number of words in quotes

def count_words_in_quotes(text):
    x = re.findall("'.'|"."", text)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count

7. Number of sentences

def count_sent(text):
    return len(nltk.sent_tokenize(text))
	
8. Count the number of unique words
def count_unique_words(text):
    return len(set(text.split()))
	
10. Count of mentions

def count_mentions(text):
    x = re.findall(r'(@w[A-Za-z0-9]*)
	', text)
    return len(x)
	
11. Count of stopwords

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

12. Calculating average word length
df['avg_wordlength'] = df['char_count']/df['word_count']

13. Calculating average sentence length
df['avg_sentlength'] = df['word_count']/df['sent_count']

Concept Learning:
In concept learning, we aim to use data to teach a machine to solve a binary classification problem. 
That is, to classify a data-point as either belonging to or not belonging to a particular concept or idea.
Features are just columns/attributes derived from our data, and so choosing features for concept learning is, in essence,
 choosing a data-viewpoint for the learning problem.
 Think aboutTrying to tell whether or not an animal is a dog by only looking at its height
 On the pytheon the other hand, if we choose to include too many features in our learning model, the noise
 and increased complexity could make the learning process significantly harder.
 
 The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning 
 and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning,
 and to improve generalization and interpretability
 
 Feature Extraction:
 Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing.
 A characteristic of these large data sets is a large number of variables that require  a lot of computing resources to process.
 Feature extraction is the name for methods that select and /or combine variables into features, effectively reducing the amount of 
 data that must be processed, while still accurately and completely 
 describing the original data set.
 
 Practical Uses of Feature Extraction:
 Autoencoders:
 
 Bag-of-Words
– A technique for natural language processing that extracts the words (features) used in a sentence, document, website, etc
I
mage Processing – Algorithms are used to detect features such as shaped, edges, or motion in a digital image or video.

 Using Regularization could certainly help reduce the risk of overfitting, but using instead Feature Extraction techniques can also lead to other types of advantages such as:
Accuracy improvements.
Overfitting risk reduction.
Speed up in training.
Improved Data Visualization.
Increase in explainability of our model.

These new reduced set of features should then be able to summarize most of the information contained in the original set of features. In this way, a summarised version of the 
original features can be created from a combination of the original set.
Another commonly used technique to reduce the number of feature in a dataset is Feature Selection

The difference between Feature Selection and Feature Extraction is that feature selection aims instead to rank the importance of the existing features in the dataset
 and discard less important ones (no new features are created).
 
 PCA is one of the most used linear dimensionality reduction technique.
 
  The difference between Feature Selection and Feature Extraction is that feature selection aims instead to rank the importance 
  of the existing features in the dataset and discard less important ones (no new features are created)
  
  The goal of feature selection in machine learning is to find the best set of features that allows one to build useful models of studied phenomena.
  A. Filter methods

B. Wrapper methods

C. Embedded methods

D. Hybrid methods

  A. Filter methods

Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. 
Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. 
The correlation is a subjective term here. For basic guidance, you can refer to the following table for defining correlation co-efficients.

	Pearson’s Correlation: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. 
	Its value varies from -1 to +1. Pearson’s correlation is given as:fs2

	LDA: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.
	ANOVA: ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.
	Chi-Square: It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.

In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in 
a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.

Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model.
For example, height and weight, household income and water consumption, mileage and price of a car, study time and leisure time, etc.

Let me take a simple example from our everyday life to explain this. Colin loves watching television while munching on chips. 
The more television he watches, the more chips he eats and the happier he gets!

Now, if we could quantify happiness and measure Colin’s happiness while he’s busy doing his favorite activity, 
which do you think would have a greater impact on his happiness? Having chips or watching television? 
That’s difficult to determine because the moment we try to measure Colin’s happiness from eating chips,
 he starts watching television. And the moment we try to measure his happiness from watching television, 
 he starts eating chips.
 
 Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual 
 effects of the independent variables on the dependent variable. 
 For example, let’s assume that in the following linear equation:

Y = W0+W1*X1+W2*X2

Coefficient W1 is the increase in Y for a unit increase in X1 while keeping X2 constant. 
But since X1 and X2 are highly correlated, changes in X1 would also cause changes in X2 and we would not be able to see their individual effect on Y.

Eating chips and watching television are highly correlated in the case of Colin and we cannot individually 
determine the impact of the individual activities on his happiness. This is the multicollinearity problem!

This makes the effects of X1 on Y difficult to distinguish from the effects of X2 on Y. ”


So why should you worry about multicollinearity in the machine learning context? Let’s answer that question next.

Multicollinearity may not affect the accuracy of the model as much. 
But we might lose reliability in determining the effects of individual features in your model – and that can be a problem when it comes to interpretability.

Inaccurate use of dummy variables can also cause a multicollinearity problem. This is called the Dummy variable trap: 


Wrapper Methods: Definition
Wrapper methods work by evaluating a subset of features using a machine learning algorithm that employs a search strategy 
to look through the space of possible feature subsets, evaluating each subset based on the quality of the performance of a given algorithm.
These methods are called greedy algorithms because they aim to find the best possible combination of features that 
result in the best performant model— which will be computationally expensive, and often impractical in the case of exhaustive search.
Practically any combination of a search strategy and a machine learning algorithm can be used as a wrapper.

Wrapper Methods: Advantages
Given the issues we encountered using filter methods, wrapper methods present two main advantages that deal with those issues:
They detect the interaction between variables
They find the optimal feature subset for the desired machine learning algorithm
The wrapper methods usually result in better predictive accuracy than filter methods.

Wrapper Methods: Process
Wrapper methods work in the following way, generally speaking:
Search for a subset of features: Using a search method (described next), we select a subset of features from the available ones.
Build a machine learning model: In this step, a chosen ML algorithm is trained on the previously-selected subset of features.
Evaluate model performance: And finally, we evaluate the newly-trained ML model with a chosen metric.
Repeat: The whole process starts again with a new subset of features, a new ML model trained, and so on.
We stop until the desired condition is met, and then we choose the best subset with the best result in the evaluation phase.


At some point in time, we need to stop searching for a subset of features. To do this, we have to put in place some
 pre-set criteria (often somewhat arbitrary), around when this searching should stop. These criteria need to be defined by the machine learning engineer. Here are a couple of examples of these criteria:
Model performance decreases.
Model performance increases.
A predefined number of features is reached.
The pre-set criteria, for example, can be metrics like ROC-AUC for classification or RMSE for regression.


Search methods
Forward Feature Selection: This method starts with no feature and adds one at a time.
Backward Feature Elimination: This method starts with all features present and removes one feature at the time.
Exhaustive Feature Selection: This method tries all possible feature combinations.
Bidirectional Search: And this last one does both forward and backward feature selection simultaneously in order to get one unique solution.


Embedded Methods: Definition
Embedded methods complete the feature selection process within the construction of the machine learning algorithm itself. 
In other words, they perform feature selection during the model training, which is why we call them embedded methods.
A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification/regression at the same time.


Embedded Methods: Advantages
The embedded method solves both issues we encountered with the filter and wrapper methods by combining their advantages. Here’s how:
They take into consideration the interaction of features like wrapper methods do.
They are faster like filter methods.
They are more accurate than filter methods.
They find the feature subset for the algorithm being trained.
They are much less prone to overfitting.

Embedded Methods: Process
Any and all embedded methods work as follows:
First, these methods train a machine learning model.
They then derive feature importance from this model, which is a measure of how much is feature important when making a prediction.
Finally, they remove non-important features using the derived feature importance.

Hybrid methods:

Hybrid Methods: Definition
The definition here is quite simple—rather than using a single approach to select feature subsets as the previous methods do,
 hybrid methods combine the different approaches to get the best possible feature subset.
The way to combine these approaches is up to the engineer, given that you have a lot of methods in your toolbox. 
You can, for example, start by performing filter methods by eliminating constant, quasi-constant and duplicated features. 
Then, in the second step, you could use wrapper methods to select the best feature subset from the previous step.
 This is just one simple, high-level approach.
 
 What Is Information Gain?
Information Gain, or IG for short, measures the reduction in entropy or surprise by splitting a dataset according to a given value of a random variable.

A larger information gain suggests a lower entropy group or groups of samples, and hence less surprise.

You might recall that information quantifies how surprising an event is in bits.
 Lower probability events have more information, higher probability events have less information. 
Entropy quantifies how much information there is in a random variable, or 
more specifically its probability distribution. A skewed distribution has a low entropy,
 whereas a distribution where events have equal probability has a larger entropy.

In information theory, we like to describe the “surprise” of an event.
 Low probability events are more surprising therefore have a larger amount of information. 
 Whereas probability distributions where the events are equally likely are more surprising and have larger entropy.
 
 The information gain is calculated for each variable in the dataset. The variable that has the largest information gain is selected to split the dataset. 
 
 Generally, a larger gain indicates a smaller entropy or less surprise.
 
 Note that minimizing the entropy is equivalent to maximizing the information gain
 the process of selecting a new attribute and partitioning the training examples is now repeated 
 for each non terminal descendant node, this time using only the training examples associated with that node. 
 Attributes that have been incorporated higher in the tree are  
 excluded, so that any given attribute can appear at most once along any path through the tree.
 
 mutual information and Chi score:
 
 Mutual information is straightforward when considering the distribution of two discrete (categorical or ordinal) variables, such as categorical 
 input and categorical output data. Nevertheless, it can be adapted for use with numerical input and categorical output.
 
 
 ANOVA f-test Feature Selection
ANOVA is an acronym for “analysis of variance” and is a parametric statistical hypothesis test for determining 
whether the means from two or more samples of data (often three or more) come from the same distribution or not.

An F-statistic, or F-test, is a class of statistical tests that calculate the ratio between variances values, 

such as the variance from two different samples or the explained and unexplained variance by a statistical test, 
like ANOVA. The ANOVA method is a type of F-statistic referred to here as an ANOVA f-test.

Importantly, ANOVA is used when one variable is numeric and one is categorical, such as numerical input
 variables and a classification target variable in a classification task.

The results of this test can be used for feature selection where those features that are independent 
of the target variable can be removed from the dataset.

hi-square measures the statistical significance of the differences between the child nodes and their parent nodes.
 It is measured as the sum of squared standardized differences between observed and expected frequencies 
of target variable for each node and is calculated using this formula-

Chi-square measures the statistical significance of the differences between the child nodes and their parent nodes.
 It is measured as the sum of squared standardized differences between observed and expected frequencies of 
target variable for each node and is calculated using this formula-

We’ve seen this before. There is a total of 20 students and out of those 10 play cricket and 10 do not. 


So, of course, the percent of students who do play cricket will be 50%. Now if we consider the “Above average”
 node here, there are 14 students in it, as the percentage of students who play cricket is 50% in the parent
 node as we discussed, the expected number of students who play cricket will of course be 7 and if you look 
 at the actual value it is 8. So now we have both the values expected values and actual values.
The expected was 7 and the actual turns out to be 8, so we can infer that the expected value is calculated 
based on the distribution of the parent node of the same class. Similarly, the expected number of students 
who do not play cricket will be 7. I want you to intuitively think about this because remember the percentage
 of students who do not play cricket in the parent node is 50% as well and then here the actual value turns out to be 6.
Similarly for the below-average node expected to play and not play will be 3. Whereas the actual values are- 2 
students play cricket and 4 do not.

Now we can calculate the chi-square using the formula mentioned above for each child node.
 Can you guess what will be the chi-square value if the actual and expected values are the same? 
 It’s actually pretty simple, it will be zero because both the actual and expected are the same and the difference will be zero.
Now if both values are the same we can generate an inference that the distribution of the child 

node is the same as there is the parent node and hence we are not improving the purity of the nodes.
 On the other hand, if the chi-square value is high it means that the distribution of child nodes is
 changing with respect to the parent node and we are going in a direction to
achieve more pure nodes hence we can say that:
Higher the chi-square value more will be the purity of the nodes after a split.

Properties of chi-square
Let’s look at some of the properties of chi-square before understanding how it actually works-
Chi-square just like Gini impurity works only with categorical variables so we cannot use it for continuous targets.
The higher the value of chi-square more the sub-nodes are different from the parent node and hence the homogeneity is more.
These are some of the properties of chi-square and you must consider these properties before choosing the right algorithm for deciding the split.

 
 
