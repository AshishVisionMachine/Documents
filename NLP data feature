Feature engineering:
 It is the process of using domain knowledge of the data to create features that make machine learning algorithms work
 
 I will be listing out a total of 15 features that we can use
1. Number of Characters
def count_chars(text):
    return len(text)

2. Number of words

def count_words(text):
    return len(text.split())
	
3. Number of capital characters

def count_capital_chars(text):
    count=0
    for i in text:
        if i.isupper():
            count+=1
    return count
	
4. Number of capital words

def count_capital_words(text):
    return sum(map(str.isupper,text.split()))
	
5. Count the number of punctuations

def count_punctuations(text):
    punctuations='!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~'
    d=dict()
    for i in punctuations:
        d[str(i)+' count']=text.count(i)
    return d 
	
6. Number of words in quotes

def count_words_in_quotes(text):
    x = re.findall("'.'|"."", text)
    count=0
    if x is None:
        return 0
    else:
        for i in x:
            t=i[1:-1]
            count+=count_words(t)
        return count

7. Number of sentences

def count_sent(text):
    return len(nltk.sent_tokenize(text))
	
8. Count the number of unique words
def count_unique_words(text):
    return len(set(text.split()))
	
10. Count of mentions

def count_mentions(text):
    x = re.findall(r'(@w[A-Za-z0-9]*)
	', text)
    return len(x)
	
11. Count of stopwords

def count_stopwords(text):
    stop_words = set(stopwords.words('english'))  
    word_tokens = word_tokenize(text)
    stopwords_x = [w for w in word_tokens if w in stop_words]
    return len(stopwords_x)

12. Calculating average word length
df['avg_wordlength'] = df['char_count']/df['word_count']

13. Calculating average sentence length
df['avg_sentlength'] = df['word_count']/df['sent_count']

Concept Learning:
In concept learning, we aim to use data to teach a machine to solve a binary classification problem. 
That is, to classify a data-point as either belonging to or not belonging to a particular concept or idea.
Features are just columns/attributes derived from our data, and so choosing features for concept learning is, in essence,
 choosing a data-viewpoint for the learning problem.
 Think aboutTrying to tell whether or not an animal is a dog by only looking at its height
 On the pytheon the other hand, if we choose to include too many features in our learning model, the noise
 and increased complexity could make the learning process significantly harder.
 
 The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning 
 and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning,
 and to improve generalization and interpretability
 
 Feature Extraction:
 Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing.
 A characteristic of these large data sets is a large number of variables that require  a lot of computing resources to process.
 Feature extraction is the name for methods that select and /or combine variables into features, effectively reducing the amount of 
 data that must be processed, while still accurately and completely 
 describing the original data set.
 
 Practical Uses of Feature Extraction:
 Autoencoders:
 
 Bag-of-Words
– A technique for natural language processing that extracts the words (features) used in a sentence, document, website, etc
I
mage Processing – Algorithms are used to detect features such as shaped, edges, or motion in a digital image or video.

 Using Regularization could certainly help reduce the risk of overfitting, but using instead Feature Extraction techniques can also lead to other types of advantages such as:
Accuracy improvements.
Overfitting risk reduction.
Speed up in training.
Improved Data Visualization.
Increase in explainability of our model.

These new reduced set of features should then be able to summarize most of the information contained in the original set of features. In this way, a summarised version of the 
original features can be created from a combination of the original set.
Another commonly used technique to reduce the number of feature in a dataset is Feature Selection

The difference between Feature Selection and Feature Extraction is that feature selection aims instead to rank the importance of the existing features in the dataset
 and discard less important ones (no new features are created).
 
 PCA is one of the most used linear dimensionality reduction technique.
 
  The difference between Feature Selection and Feature Extraction is that feature selection aims instead to rank the importance 
  of the existing features in the dataset and discard less important ones (no new features are created)
  
  The goal of feature selection in machine learning is to find the best set of features that allows one to build useful models of studied phenomena.
  A. Filter methods

B. Wrapper methods

C. Embedded methods

D. Hybrid methods

  A. Filter methods

Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. 
Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. 
The correlation is a subjective term here. For basic guidance, you can refer to the following table for defining correlation co-efficients.

	Pearson’s Correlation: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. 
	Its value varies from -1 to +1. Pearson’s correlation is given as:fs2

	LDA: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.
	ANOVA: ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.
	Chi-Square: It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.

In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in 
a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.

Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model.
For example, height and weight, household income and water consumption, mileage and price of a car, study time and leisure time, etc.

Let me take a simple example from our everyday life to explain this. Colin loves watching television while munching on chips. 
The more television he watches, the more chips he eats and the happier he gets!

Now, if we could quantify happiness and measure Colin’s happiness while he’s busy doing his favorite activity, 
which do you think would have a greater impact on his happiness? Having chips or watching television? 
That’s difficult to determine because the moment we try to measure Colin’s happiness from eating chips,
 he starts watching television. And the moment we try to measure his happiness from watching television, 
 he starts eating chips.
 
 Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual 
 effects of the independent variables on the dependent variable. 
 For example, let’s assume that in the following linear equation:

Y = W0+W1*X1+W2*X2

Coefficient W1 is the increase in Y for a unit increase in X1 while keeping X2 constant. 
But since X1 and X2 are highly correlated, changes in X1 would also cause changes in X2 and we would not be able to see their individual effect on Y.

Eating chips and watching television are highly correlated in the case of Colin and we cannot individually 
determine the impact of the individual activities on his happiness. This is the multicollinearity problem!

This makes the effects of X1 on Y difficult to distinguish from the effects of X2 on Y. ”


So why should you worry about multicollinearity in the machine learning context? Let’s answer that question next.

Multicollinearity may not affect the accuracy of the model as much. 
But we might lose reliability in determining the effects of individual features in your model – and that can be a problem when it comes to interpretability.

Inaccurate use of dummy variables can also cause a multicollinearity problem. This is called the Dummy variable trap: 


Wrapper Methods: Definition
Wrapper methods work by evaluating a subset of features using a machine learning algorithm that employs a search strategy 
to look through the space of possible feature subsets, evaluating each subset based on the quality of the performance of a given algorithm.
These methods are called greedy algorithms because they aim to find the best possible combination of features that 
result in the best performant model— which will be computationally expensive, and often impractical in the case of exhaustive search.
Practically any combination of a search strategy and a machine learning algorithm can be used as a wrapper.

Wrapper Methods: Advantages
Given the issues we encountered using filter methods, wrapper methods present two main advantages that deal with those issues:
They detect the interaction between variables
They find the optimal feature subset for the desired machine learning algorithm
The wrapper methods usually result in better predictive accuracy than filter methods.

Wrapper Methods: Process
Wrapper methods work in the following way, generally speaking:
Search for a subset of features: Using a search method (described next), we select a subset of features from the available ones.
Build a machine learning model: In this step, a chosen ML algorithm is trained on the previously-selected subset of features.
Evaluate model performance: And finally, we evaluate the newly-trained ML model with a chosen metric.
Repeat: The whole process starts again with a new subset of features, a new ML model trained, and so on.
We stop until the desired condition is met, and then we choose the best subset with the best result in the evaluation phase.


At some point in time, we need to stop searching for a subset of features. To do this, we have to put in place some
 pre-set criteria (often somewhat arbitrary), around when this searching should stop. These criteria need to be defined by the machine learning engineer. Here are a couple of examples of these criteria:
Model performance decreases.
Model performance increases.
A predefined number of features is reached.
The pre-set criteria, for example, can be metrics like ROC-AUC for classification or RMSE for regression.


Search methods
Forward Feature Selection: This method starts with no feature and adds one at a time.
Backward Feature Elimination: This method starts with all features present and removes one feature at the time.
Exhaustive Feature Selection: This method tries all possible feature combinations.
Bidirectional Search: And this last one does both forward and backward feature selection simultaneously in order to get one unique solution.


Embedded Methods: Definition
Embedded methods complete the feature selection process within the construction of the machine learning algorithm itself. 
In other words, they perform feature selection during the model training, which is why we call them embedded methods.
A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification/regression at the same time.


Embedded Methods: Advantages
The embedded method solves both issues we encountered with the filter and wrapper methods by combining their advantages. Here’s how:
They take into consideration the interaction of features like wrapper methods do.
They are faster like filter methods.
They are more accurate than filter methods.
They find the feature subset for the algorithm being trained.
They are much less prone to overfitting.

Embedded Methods: Process
Any and all embedded methods work as follows:
First, these methods train a machine learning model.
They then derive feature importance from this model, which is a measure of how much is feature important when making a prediction.
Finally, they remove non-important features using the derived feature importance.